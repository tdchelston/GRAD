{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2ee642-8291-4b2e-9cf9-542fbf832779",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Tyler Chelston\n",
    "\n",
    "CS584 Project 3\n",
    "\n",
    "\n",
    "This project presents the development of an English to Spanish language translator \n",
    "using a Seq2Seq neural network model. Essential libraries like pandas and tensorflow \n",
    "are utilized to handle and process a dataset of 140,000 bilingual sentence pairs. \n",
    "The data undergoes extensive cleaning, including normalization and punctuation removal, \n",
    "to ensure quality input for the model. \n",
    "\n",
    "The Seq2Seq model, comprising an encoder and decoder with LSTM layers, is trained on this data. \n",
    "Key features include vocabulary building, character-to-index mappings, and hyperparameter tuning. \n",
    "The final product is an interactive program allowing users to input an English sentence \n",
    "and receive its Spanish translation, demonstrating the model's practical application in \n",
    "natural language processing.\n",
    "\n",
    "The less complicated words and phrases had cleaner translations than the more complicated. I believe that decreasing the batch size and adding more epochs could help make it more accurate\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d212de-6972-48b3-8534-042d16c56a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/User/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/User/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/User/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/User/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/User/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/User/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Load and prepare data\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().split('\\n')\n",
    "\n",
    "    english_sentences = []\n",
    "    spanish_sentences = []\n",
    "\n",
    "    for line in lines:\n",
    "        if '\\t' in line:\n",
    "            parts = line.split('\\t')\n",
    "            eng, spa = parts[0], parts[1]  # Take only the first two parts\n",
    "            english_sentences.append(eng)\n",
    "            spanish_sentences.append('\\t' + spa + '\\n')\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'English': english_sentences,\n",
    "        'Spanish': spanish_sentences\n",
    "    })\n",
    "\n",
    "    data['English'] = data['English'].apply(clean_sentence)\n",
    "    data['Spanish'] = data['Spanish'].apply(clean_sentence)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Normalize characters (to ASCII)\n",
    "    sentence = unicodedata.normalize('NFD', sentence).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove punctuation\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "    # Perform case-folding (convert to lowercase)\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Remove non-printable characters\n",
    "    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n",
    "\n",
    "    # Optional: Remove digits (if you want to keep only alphabetic words)\n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "# Tokenization and sequence padding\n",
    "def tokenize_and_pad(data, tokenizer, max_length):\n",
    "    sequences = tokenizer.texts_to_sequences(data)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "# Function to generate the decoded sentence\n",
    "def decode_sequence(input_seq):\n",
    "    # Start with initial states from the encoder model\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Start the sequence with the start token '\\t'\n",
    "    target_seq = np.zeros((1, 1, num_spa_characters))\n",
    "    target_seq[0, 0, spa_tokenizer.word_index['\\t']] = 1\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index.get(sampled_token_index)\n",
    "\n",
    "        if sampled_char == '\\n':\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += sampled_char\n",
    "\n",
    "            # Update the target sequence to be the last predicted character\n",
    "            target_seq = np.zeros((1, 1, num_spa_characters))\n",
    "            if sampled_token_index in spa_tokenizer.word_index.values():\n",
    "                target_seq[0, 0, sampled_token_index] = 1\n",
    "\n",
    "            # Update states\n",
    "            states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()\n",
    "\n",
    "\n",
    "# Function to convert English sentence to sequence\n",
    "def sentence_to_sequence(sentence, tokenizer, max_length):\n",
    "    sentence = clean_sentence(sentence)\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    return padded_sequence\n",
    "\n",
    "# Function to translate a sentence given its index in the dataset\n",
    "def translate_sentence_by_index(index, data, tokenizer, max_length, decode_sequence):\n",
    "    # Fetch the sentence\n",
    "    sentence_to_translate = data.iloc[index]['English']\n",
    "    print(f\"Original sentence: {sentence_to_translate}\")\n",
    "\n",
    "    # Convert sentence to sequence\n",
    "    sequence = sentence_to_sequence(sentence_to_translate, tokenizer, max_length)\n",
    "    sequence_one_hot = to_categorical(sequence, num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "    # Translate the sequence\n",
    "    translation = decode_sequence(sequence_one_hot)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2866bd59-43c5-4135-bebc-127d24d51871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  English     Spanish\n",
      "0      go      \\tve\\n\n",
      "1      go    \\tvete\\n\n",
      "2      go    \\tvaya\\n\n",
      "3      go  \\tvayase\\n\n",
      "4      hi    \\thola\\n\n",
      "encoder_input_data shape: (140000, 79, 29)\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "file_path = 'Downloads/spa-eng/spa.txt'\n",
    "data = load_dataset(file_path)\n",
    "print(data.head())\n",
    "\n",
    "if len(data) > 140000:\n",
    "    data = data[:140000]\n",
    "\n",
    "# Clean Data and remove extra text\n",
    "data['English'] = data['English'].str.split('.').str[0] + '.'\n",
    "data['Spanish'] = data['Spanish'].str.split('.').str[0] + '.'\n",
    "\n",
    "# Re-tokenize and prepare data\n",
    "eng_tokenizer = Tokenizer(char_level=True)\n",
    "eng_tokenizer.fit_on_texts(data['English'])\n",
    "spa_tokenizer = Tokenizer(char_level=True)\n",
    "spa_tokenizer.fit_on_texts(data['Spanish'])\n",
    "\n",
    "# Create reverse mapping from index to character for the Spanish tokenizer\n",
    "reverse_target_char_index = dict((i, char) for char, i in spa_tokenizer.word_index.items())\n",
    "\n",
    "# Define the number of unique characters\n",
    "num_eng_characters = len(eng_tokenizer.word_index) + 1  # Number of unique English characters\n",
    "num_spa_characters = len(spa_tokenizer.word_index) + 1  # Number of unique Spanish characters\n",
    "\n",
    "# Convert text to sequences and pad\n",
    "max_eng_length = max(len(seq) for seq in eng_tokenizer.texts_to_sequences(data['English']))\n",
    "max_spa_length = max(len(seq) for seq in spa_tokenizer.texts_to_sequences(data['Spanish']))\n",
    "\n",
    "# Convert text to sequences (before one-hot encoding)\n",
    "encoder_sequences = tokenize_and_pad(data['English'], eng_tokenizer, max_eng_length)\n",
    "decoder_sequences = tokenize_and_pad(data['Spanish'], spa_tokenizer, max_spa_length)\n",
    "\n",
    "# One-hot encode the input sequences\n",
    "encoder_input_data = to_categorical(encoder_sequences, num_classes=num_eng_characters)\n",
    "print(\"encoder_input_data shape:\", encoder_input_data.shape)\n",
    "# One-hot encode the decoder input sequences\n",
    "decoder_input_data = to_categorical(decoder_sequences, num_classes=num_spa_characters)\n",
    "\n",
    "# One-hot encode the decoder target sequences\n",
    "decoder_target_data = np.zeros((len(data), max_spa_length, num_spa_characters), dtype='float32')\n",
    "\n",
    "for i, seq in enumerate(decoder_sequences):\n",
    "    for t, char_idx in enumerate(seq):\n",
    "        if t > 0:  # Skipping the start token represented by '\\t'\n",
    "            decoder_target_data[i, t - 1, char_idx] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dade50bf-235f-4d08-8890-562e2c85ffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few tokenized English sequences: [[18, 4, 11], [18, 4, 11], [18, 4, 11], [18, 4, 11], [9, 6, 11]]\n",
      "Max length of English sequences: 79\n",
      "First few cleaned English sentences: 0    go.\n",
      "1    go.\n",
      "2    go.\n",
      "3    go.\n",
      "4    hi.\n",
      "Name: English, dtype: object\n",
      "Tokenizer word index: {' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'i': 6, 'n': 7, 's': 8, 'h': 9, 'r': 10, '.': 11, 'l': 12, 'd': 13, 'm': 14, 'y': 15, 'u': 16, 'w': 17, 'g': 18, 'c': 19, 'f': 20, 'p': 21, 'b': 22, 'k': 23, 'v': 24, 'j': 25, 'x': 26, 'q': 27, 'z': 28}\n",
      "First few tokenized English sequences (raw): [[18, 4, 11], [18, 4, 11], [18, 4, 11], [18, 4, 11], [9, 6, 11]]\n",
      "encoder_input_data shape: (140000, 79, 29)\n",
      "decoder_input_data shape: (140000, 113, 31)\n",
      "decoder_target_data shape: (140000, 113, 31)\n"
     ]
    }
   ],
   "source": [
    "# Check the first few tokenized English sequences\n",
    "tokenized_eng = eng_tokenizer.texts_to_sequences(data['English'])\n",
    "print(\"First few tokenized English sequences:\", tokenized_eng[:5])\n",
    "\n",
    "# Check max length of English sequences\n",
    "max_eng_length = max(len(seq) for seq in tokenized_eng)\n",
    "print(\"Max length of English sequences:\", max_eng_length)\n",
    "\n",
    "# If max_eng_length is still 0, inspect the cleaned English sentences\n",
    "print(\"First few cleaned English sentences:\", data['English'].head())\n",
    "\n",
    "# Print the tokenizer's learned characters\n",
    "print(\"Tokenizer word index:\", eng_tokenizer.word_index)\n",
    "\n",
    "# Test tokenizing without cleaning\n",
    "eng_tokenizer.fit_on_texts(data['English'].str.replace('\\t', '').str.replace('\\n', ''))\n",
    "tokenized_eng_raw = eng_tokenizer.texts_to_sequences(data['English'].str.replace('\\t', '').str.replace('\\n', ''))\n",
    "print(\"First few tokenized English sequences (raw):\", tokenized_eng_raw[:5])\n",
    "\n",
    "# Print shapes\n",
    "print(\"encoder_input_data shape:\", encoder_input_data.shape)\n",
    "print(\"decoder_input_data shape:\", decoder_input_data.shape)\n",
    "print(\"decoder_target_data shape:\", decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e36540-48f0-433b-baa5-2a7d444cb34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample English sentences:\n",
      "0       go.\n",
      "1       go.\n",
      "2       go.\n",
      "3       go.\n",
      "4       hi.\n",
      "5      run.\n",
      "6      run.\n",
      "7      run.\n",
      "8      run.\n",
      "9      run.\n",
      "10     run.\n",
      "11     run.\n",
      "12     who.\n",
      "13     wow.\n",
      "14    duck.\n",
      "15    fire.\n",
      "16    fire.\n",
      "17    fire.\n",
      "18    help.\n",
      "19    help.\n",
      "Name: English, dtype: object\n",
      "Tokenizer word index: {' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'i': 6, 'n': 7, 's': 8, 'h': 9, 'r': 10, '.': 11, 'l': 12, 'd': 13, 'm': 14, 'y': 15, 'u': 16, 'w': 17, 'g': 18, 'c': 19, 'f': 20, 'p': 21, 'b': 22, 'k': 23, 'v': 24, 'j': 25, 'x': 26, 'q': 27, 'z': 28}\n",
      "Re-initialized tokenizer word index: {' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'i': 6, 'n': 7, 's': 8, 'h': 9, 'r': 10, '.': 11, 'l': 12, 'd': 13, 'm': 14, 'y': 15, 'u': 16, 'w': 17, 'g': 18, 'c': 19, 'f': 20, 'p': 21, 'b': 22, 'k': 23, 'v': 24, 'j': 25, 'x': 26, 'q': 27, 'z': 28}\n",
      "First few tokenized English sequences after re-initialization: [[18, 4, 11], [18, 4, 11], [18, 4, 11], [18, 4, 11], [9, 6, 11]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the actual English sentences\n",
    "print(\"Sample English sentences:\")\n",
    "print(data['English'].head(20))\n",
    "\n",
    "# Check the tokenizer's word index\n",
    "print(\"Tokenizer word index:\", eng_tokenizer.word_index)\n",
    "\n",
    "# Re-initialize and re-fit the tokenizer\n",
    "eng_tokenizer = Tokenizer(char_level=True)\n",
    "eng_tokenizer.fit_on_texts(data['English'])\n",
    "\n",
    "# Check the tokenizer's word index again\n",
    "print(\"Re-initialized tokenizer word index:\", eng_tokenizer.word_index)\n",
    "\n",
    "# Tokenize and check the sequences again\n",
    "tokenized_eng = eng_tokenizer.texts_to_sequences(data['English'])\n",
    "print(\"First few tokenized English sequences after re-initialization:\", tokenized_eng[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3ea9c69-2733-4637-a8ac-6fe4c7ec746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input data: (140000, 79, 29)\n",
      "Number of unique English characters: 29\n"
     ]
    }
   ],
   "source": [
    "# After tokenizing and one-hot encoding\n",
    "print(\"Shape of encoder input data:\", encoder_input_data.shape)\n",
    "print(\"Number of unique English characters:\", num_eng_characters)\n",
    "\n",
    "# Adjust the model's input layer\n",
    "encoder_inputs = Input(shape=(None, num_eng_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494d1869-e777-488e-ab75-c58bdf9c0893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max English sequence length: 79\n",
      "WARNING:tensorflow:From /Users/User/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "latent_dim = 256\n",
    "\n",
    "max_eng_length = max(len(seq) for seq in eng_tokenizer.texts_to_sequences(data['English']))\n",
    "print(\"Max English sequence length:\", max_eng_length)\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_eng_length, num_eng_characters))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None, len(spa_tokenizer.word_index) + 1))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(spa_tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56932a74-15a9-4b84-9aeb-36bfb03f05b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: (None, None, 31)\n",
      "decoder_target_data shape: (140000, 113, 31)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Check the Shape\n",
    "print(\"Model output shape:\", model.output_shape)\n",
    "print(\"decoder_target_data shape:\", decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7d5516-fb64-420c-a3dd-13dcae571ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112000 samples, validate on 28000 samples\n",
      "WARNING:tensorflow:From /Users/User/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/User/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 01:12:51.366412: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\n",
      "2023-11-27 01:12:51.367092: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 10. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111872/112000 [============================>.] - ETA: 4s - loss: 0.6052\n",
      "Epoch 00001: val_loss improved from inf to 0.89650, saving model to best_model.h5\n",
      "112000/112000 [==============================] - 4366s 39ms/sample - loss: 0.6050 - val_loss: 0.8965\n",
      "Epoch 2/10\n",
      "111872/112000 [============================>.] - ETA: 4s - loss: 0.4350\n",
      "Epoch 00002: val_loss improved from 0.89650 to 0.77204, saving model to best_model.h5\n",
      "112000/112000 [==============================] - 4200s 38ms/sample - loss: 0.4349 - val_loss: 0.7720\n",
      "Epoch 3/10\n",
      "111872/112000 [============================>.] - ETA: 4s - loss: 0.3775\n",
      "Epoch 00003: val_loss improved from 0.77204 to 0.70000, saving model to best_model.h5\n",
      "112000/112000 [==============================] - 4418s 39ms/sample - loss: 0.3775 - val_loss: 0.7000\n",
      "Epoch 4/10\n",
      "111872/112000 [============================>.] - ETA: 4s - loss: 0.3374\n",
      "Epoch 00004: val_loss improved from 0.70000 to 0.64147, saving model to best_model.h5\n",
      "112000/112000 [==============================] - 4217s 38ms/sample - loss: 0.3374 - val_loss: 0.6415\n",
      "Epoch 5/10\n",
      "111872/112000 [============================>.] - ETA: 4s - loss: 0.3092\n",
      "Epoch 00005: val_loss improved from 0.64147 to 0.60279, saving model to best_model.h5\n",
      "112000/112000 [==============================] - 4369s 39ms/sample - loss: 0.3092 - val_loss: 0.6028\n",
      "Epoch 6/10\n",
      "111872/112000 [============================>.] - ETA: 4s - loss: 0.2875\n",
      "Epoch 00006: val_loss improved from 0.60279 to 0.57301, saving model to best_model.h5\n",
      "112000/112000 [==============================] - 4271s 38ms/sample - loss: 0.2875 - val_loss: 0.5730\n",
      "Epoch 7/10\n",
      "111872/112000 [============================>.] - ETA: 8s - loss: 0.2711 \n",
      "Epoch 00007: val_loss improved from 0.57301 to 0.55520, saving model to best_model.h5\n",
      "112000/112000 [==============================] - 7779s 69ms/sample - loss: 0.2710 - val_loss: 0.5552\n",
      "Epoch 8/10\n",
      "111872/112000 [============================>.] - ETA: 8s - loss: 0.2722 \n",
      "Epoch 00008: val_loss did not improve from 0.55520\n",
      "112000/112000 [==============================] - 7527s 67ms/sample - loss: 0.2722 - val_loss: 0.5664\n",
      "Epoch 9/10\n",
      "111872/112000 [============================>.] - ETA: 4s - loss: 0.2583\n",
      "Epoch 00009: val_loss improved from 0.55520 to 0.53639, saving model to best_model.h5\n",
      "112000/112000 [==============================] - 3925s 35ms/sample - loss: 0.2582 - val_loss: 0.5364\n",
      "Epoch 10/10\n",
      "111872/112000 [============================>.] - ETA: 4s - loss: 0.2484\n",
      "Epoch 00010: val_loss improved from 0.53639 to 0.52457, saving model to best_model.h5\n",
      "112000/112000 [==============================] - 3911s 35ms/sample - loss: 0.2484 - val_loss: 0.5246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcea6876550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "# Model checkpoint callback\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "# Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "\n",
    "# Compile & train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, \n",
    "          batch_size=128, \n",
    "          epochs=10, \n",
    "          validation_split=0.2,\n",
    "          callbacks=[early_stopping, model_checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc07fc6-3aa8-40ed-b5f0-c3e27407c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2e72eb9-c825-407f-8ee7-350e0ba0ad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an integer up to 140000 to see a sentence translated:  10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: i hate tomatoes.\n",
      "Translated sentence: odio a tom\n"
     ]
    }
   ],
   "source": [
    "# Test the model & Prompt user for input\n",
    "try:\n",
    "    user_input = int(input(\"Enter an integer up to 140000 to see a sentence translated: \"))\n",
    "    if 0 <= user_input < len(data):\n",
    "        translated_sentence = translate_sentence_by_index(user_input, data, eng_tokenizer, max_eng_length, decode_sequence)\n",
    "        print(\"Translated sentence:\", translated_sentence)\n",
    "    else:\n",
    "        print(\"Input integer is out of range. Please enter a number between 0 and\", len(data) - 1)\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter an integer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d18198c-8ea5-40e4-9c0e-738c8c8b54d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an integer up to 140000 to see a sentence translated:  110000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: they didnt feel like playing any more.\n",
      "Translated sentence: ellos se consejaron el pelo de la cama\n"
     ]
    }
   ],
   "source": [
    "# Test the model & Prompt user for input\n",
    "try:\n",
    "    user_input = int(input(\"Enter an integer up to 140000 to see a sentence translated: \"))\n",
    "    if 0 <= user_input < len(data):\n",
    "        translated_sentence = translate_sentence_by_index(user_input, data, eng_tokenizer, max_eng_length, decode_sequence)\n",
    "        print(\"Translated sentence:\", translated_sentence)\n",
    "    else:\n",
    "        print(\"Input integer is out of range. Please enter a number between 0 and\", len(data) - 1)\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter an integer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "678759b4-3dc5-4b9b-9ec2-d020504b6d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an integer up to 140000 to see a sentence translated:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: help.\n",
      "Translated sentence: se despertado\n"
     ]
    }
   ],
   "source": [
    "# Test the model & Prompt user for input\n",
    "try:\n",
    "    user_input = int(input(\"Enter an integer up to 140000 to see a sentence translated: \"))\n",
    "    if 0 <= user_input < len(data):\n",
    "        translated_sentence = translate_sentence_by_index(user_input, data, eng_tokenizer, max_eng_length, decode_sequence)\n",
    "        print(\"Translated sentence:\", translated_sentence)\n",
    "    else:\n",
    "        print(\"Input integer is out of range. Please enter a number between 0 and\", len(data) - 1)\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter an integer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623a53b-8ac0-40e3-a40e-5bf255e516a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
